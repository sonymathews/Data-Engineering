# Project Description, Context and Goals:
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.
As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights into what songs their users are listening to.

# How to run the Python scripts:
## Technology and Toolset skills needed:
    Python, SQL, Jupyter Notebook, AWS, Redshift
## Instructions
1. In Jupeternotebook - open notebook "create infra_scripts.ipynb" to run the acripts to create REDSHIFT cluster and all other resources. Make sure the ACCESS KEY/SECRET are added into the cfg file before executing the sripts
2. Open terminal window in the workspace and run command "python create_tables.py" to create the database and tables.
3. Run "python etl.py" to execute the ETL pipeline and load the data into the tables.
4. In Jupeternotebook - open notebook "test.ipynb" to run the tests and observe the results and outputs.
5. In Jupeternotebook - open notebook "delete infra_scripts.ipynb" to run the acripts to delete REDSHIFT cluster and all other resources. Make sure the ACCESS KEY/SECRET are added into the cfg file before executing the sripts

# Project Datasets:
There are three datasets that reside in S3
- Song data in:  s3://udacity-dend/song_data
    Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
- Log data in: s3://udacity-dend/log_data
    The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. The log files in the dataset you'll be working with are partitioned by year and month.  
- MetaData for the Log data in: s3://udacity-dend/log_json_path.json. This ensures that the log files can be correctly loaded into AWS.

# Files in the Repository:
create infra_scripts.ipynb - jupyter notebook which contains details of the scripts needed to be executed to create the required REDSHIFT CLUSTER and DWH
delete infra_scripts.ipynb - jupyter notebook which contains details of the scripts needed to be executed to delete the required REDSHIFT CLUSTER and DWH
create_tables.py: python files that runs the SQL queries python scripts. Its where you'll create your fact and dimension tables for the star schema in Redshift.
etl.py: python scripts that constructs the ETL pipeline and execute them either line by line or in one go.
sql_queries.py: SQL queries that are used to create the tables and insert data. Is where you'll define you SQL statements, which will be imported into the two other files above.
README.md is where you'll provide discussion on your process and decisions for this ETL pipeline.

# Database Schema Design:
## Fact Table:
    songplays - records in event data associated with song plays  
                Columns: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
  
## Dimension Tables
    users - users in the app
            Columns: user_id, first_name, last_name, gender, level
    
    songs - songs that are played and held in music database
            Columns: song_id, title, artist_id, year, duration
    
    artists - artists whose songs in music database are being played by the user
              Columns: artist_id, name, location, latitude, longitude
    
    time - timestamps of records in songplays broken down into specific units
           Columns: start_time, hour, day, week, month, year, weekday


# ETL pipeline:
1. Create table schemas. after which create and launch the AWS redshift cluster and ensure that the cluster has access to the S3 files above.
2. Add Redshift database and IAM role info into the config file dwh.cfg
3. Next Step is to run the ETL file so that the data in S3 can be loaded into the staging tables in Redshift.
4. Once the staging files are loaded execute the ETL step to load the data from staging into the Analytical Fact and Dimension tables created in step 1.
5. Execute the analytical queries to ensure that the data has been correctly loaded and the results matches what's expected.
6. Finally delete the AWS Redshift Cluster so that the DWH can be deleted.